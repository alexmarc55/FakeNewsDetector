{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffadc81-748b-44cc-888d-623c82201a4d",
   "metadata": {},
   "source": [
    "## Load Dataset and add label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d5c98b-f9a9-4724-a7ac-2dd3bdf27f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d110a06-cc95-441c-ae55-c62f9227e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_ds = pd.read_csv('dataset/fake.csv')\n",
    "true_ds = pd.read_csv('dataset/true.csv')\n",
    "\n",
    "fake_ds = fake_ds.dropna(how='all')\n",
    "true_ds = true_ds.dropna(how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56c7f45-fb8a-4d70-99a8-f0feb4ee4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_ds = fake_ds.assign(label=1)\n",
    "true_ds = true_ds.assign(label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58bd76c7-e3a3-492b-8e26-e47e1aa6cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One dataset\n",
    "ds = pd.concat([fake_ds, true_ds],axis=0)\n",
    "# random shuffle\n",
    "ds = ds.sample(frac=1)\n",
    "\n",
    "# drop the index colunm\n",
    "ds.reset_index(inplace = True)\n",
    "ds.drop([\"index\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba919a-19d8-4f61-a666-8c7c3e74535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# despartim textele de labels\n",
    "x = ds['text']\n",
    "y = ds['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e82705-31f0-461c-9891-1e4cd6418923",
   "metadata": {},
   "source": [
    "## Antrenam diferite modele pe setul de date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23785adf-6e73-41e4-a54a-0821def2bf10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Training RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a78c4-273e-4460-b872-0533c8491c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pregatim datele pentru antrenament\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "le = LabelEncoder()\n",
    "cv = CountVectorizer()\n",
    "Y = le.fit_transform(y)\n",
    "\n",
    "text_list = []\n",
    "for text in x:\n",
    "    text_list.append(text)\n",
    "X = cv.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977cb72d-68ed-4c38-9c70-c1646b0086f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impartim datele in seturi de antrenament si in seturi de validare\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=.15,random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d181d-169b-4507-949f-7d7eff091e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antrenam RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 100, random_state = 41, verbose = 10, n_jobs=-1)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f166806-e41b-4609-bd32-929acbcbacd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificam acuratetea modelului\n",
    "y_pred = clf.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of the Random Forest classifier: {accuracy:.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d604b6-3a54-488f-88f3-1143affa6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvam modelul\n",
    "from joblib import dump\n",
    "dump(clf, 'models/RandomForest/trained_model.pkl')\n",
    "dump(cv, 'models/RandomForest/count_vectorizer.pkl')\n",
    "dump(le, 'models/RandomForest/label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea55a7-c5f0-47a0-b6b8-b3098a8576cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. Training NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96d9c2-822b-4b21-94d1-05d626e0f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pregatim datele pentru antrenament\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "le = LabelEncoder()\n",
    "cv = CountVectorizer()\n",
    "Y = le.fit_transform(y)\n",
    "\n",
    "text_list = []\n",
    "for text in x:\n",
    "    text_list.append(text)\n",
    "X = cv.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7735d-70bb-4259-b860-9cb05c03be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impartim datele in seturi de antrenament si in seturi de validare\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=.15,random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694e783-661c-468f-8ad2-735ae908ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antrenam NaiveBayes\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "clf = ComplementNB()\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e639f83-c170-4aa6-9bd5-17a90f4bec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificam acuratetea modelului\n",
    "y_pred = clf.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of the Random Forest classifier: {accuracy:.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458bfd1f-34ee-454b-b848-b62dd843b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvam modelul\n",
    "from joblib import dump\n",
    "dump(clf, 'models/NaiveBayes/trained_model.pkl')\n",
    "dump(cv, 'models/NaiveBayes/count_vectorizer.pkl')\n",
    "dump(le, 'models/NaiveBayes/label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363d7b5-e79c-422e-9629-112d16727f02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd377e2-1072-4268-bdd2-91b6498f7336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pregatim datele pentru antrenament\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "le = LabelEncoder()\n",
    "cv = CountVectorizer()\n",
    "Y = le.fit_transform(y)\n",
    "\n",
    "text_list = []\n",
    "for text in x:\n",
    "    text_list.append(text)\n",
    "X = cv.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16955eba-e2e6-4f01-952a-2c850815e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impartim datele in seturi de antrenament si in seturi de validare\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=.15,random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f13d7-a335-44d2-837f-ad16669bf4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antrenam LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=1500, verbose=10, n_jobs=-1)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d99926-f531-407e-a871-177cf8ae62f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificam acuratetea modelului\n",
    "y_pred = clf.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of the Random Forest classifier: {accuracy:.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2bf3f-74db-4597-930a-6a12722d1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvam modelul\n",
    "from joblib import dump\n",
    "dump(clf, 'models/LogisticRegression/trained_model.pkl')\n",
    "dump(cv, 'models/LogisticRegression/count_vectorizer.pkl')\n",
    "dump(le, 'models/LogisticRegression/label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f8d544-99ec-4539-b31c-a0f05e9dd50f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4. Training SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07485e8f-cc94-406d-a8cd-c36ec41cf2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pregatim datele pentru antrenament\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "le = LabelEncoder()\n",
    "cv = CountVectorizer()\n",
    "Y = le.fit_transform(y)\n",
    "\n",
    "text_list = []\n",
    "for text in x:\n",
    "    text_list.append(text)\n",
    "X = cv.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de126097-b72f-4dda-a8f5-c0f8b895509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impartim datele in seturi de antrenament si in seturi de validare\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=.15,random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae91da9-7ce4-4def-9874-c3291931a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(        \n",
    "        max_iter=20_000,\n",
    "        dual=False)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51746083-6ecd-4272-b186-9f5bfa37afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificam acuratetea modelului\n",
    "y_pred = clf.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of the Random Forest classifier: {accuracy:.4f}\")\n",
    "print()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39480abc-ebb7-4857-8c82-3fc53bfc4df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvam modelul\n",
    "from joblib import dump\n",
    "dump(clf, 'models/SVM/trained_model.pkl')\n",
    "dump(cv, 'models/SVM/count_vectorizer.pkl')\n",
    "dump(le, 'models/SVM/label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d40374d-3548-47fe-9f2b-4ede44e7adc5",
   "metadata": {},
   "source": [
    "### 5. FineTune MiniLM-L3-H384-uncased\n",
    "MiniLM-L3-H384-uncased is  a 3 layer version of microsoft/MiniLM-L12-H384-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "721d9c5e-b9a9-4df0-b4d4-0d42ad10b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform pandas DataFrame to dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "dataset = Dataset.from_pandas(ds)\n",
    "splits = dataset.train_test_split(test_size=0.15, seed=42)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': splits['train'],\n",
    "    'test': splits['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b9552c-5e44-4d12-88c5-b2684b7e9a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd071851b72e492eab578ebbf4189ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ad5a6ca16d4379831114bbd4524282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize the data\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nreimers/MiniLM-L3-H384-uncased\")\n",
    "\n",
    "def preprocess_function(date):\n",
    "    return tokenizer(date[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_ds = dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e898d01-eab3-4f4b-8cb6-c3abd47cad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for efficiency\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6d1f4-4eb9-4cea-924d-84d9d32d25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting ready for training #1: optimizer\n",
    "id2label = {0: \"TRUE\", 1: \"FAKE\"}\n",
    "label2id = {\"TRUE\": 0, \"FAKE\": 1}\n",
    "\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "BATCHES_PER_EPOCH = len(tokenized_ds[\"train\"]) // BATCH_SIZE\n",
    "total_train_steps = int(BATCHES_PER_EPOCH * EPOCHS)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195001b-a0d2-495d-93d0-74c326f1603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting ready for training #2: load model and convert ds to tf.dataset\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"nreimers/MiniLM-L3-H384-uncased\",\n",
    "    from_pt=True,\n",
    "    num_labels=2, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f863fba-a21f-4722-9a11-2bf57841446f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# getting ready for training #3: compile the model\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b5bbfd-bdae-4caa-b415-6c8656e8ea3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# getting ready for training #4: Callbacks\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "from tf_keras.callbacks import TensorBoard\n",
    "from tf_keras.callbacks import BackupAndRestore\n",
    "from tf_keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def compute_metrics(eval_predictions):\n",
    "    predictions, labels = eval_predictions\n",
    "    # For classification tasks, we need to get the argmax\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # You'll need a metric object - using accuracy from sklearn for simplicity\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn=compute_metrics, eval_dataset=tf_validation_set\n",
    ")\n",
    "\n",
    "periodic_checkpoint = ModelCheckpoint(\n",
    "    filepath=\"./checkpoint/model_epoch_{epoch:03d}.keras\",\n",
    "    save_freq= 5 * BATCHES_PER_EPOCH,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "backup_callback = BackupAndRestore(\n",
    "    backup_dir=\"./training_backups\",\n",
    "    save_freq= 1 * BATCHES_PER_EPOCH\n",
    ")\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./tensorboard_logs/logs\")\n",
    "callbacks = [metric_callback, tensorboard_callback, backup_callback, periodic_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05732bf5-88fb-4803-a218-0c9dd9764913",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Set memory growth to prevent OOM errors\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e16ea767-534a-4630-a01f-075e7641b366",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746469873.666898   34391 service.cc:152] XLA service 0x7efc08330c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1746469873.667038   34391 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 OEM, Compute Capability 8.6\n",
      "2025-05-05 21:31:13.751781: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1746469873.844954   34390 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "I0000 00:00:1746469874.102499   34391 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4150/4150 [==============================] - 953s 227ms/step - loss: 0.1096 - val_loss: 0.0638 - accuracy: 0.9788 - precision: 0.9788 - recall: 0.9788 - f1: 0.9788\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 21:47:00.771188: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4150/4150 [==============================] - 834s 201ms/step - loss: 0.0597 - val_loss: 0.0607 - accuracy: 0.9800 - precision: 0.9801 - recall: 0.9800 - f1: 0.9800\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 22:00:54.371954: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4150/4150 [==============================] - 824s 198ms/step - loss: 0.0459 - val_loss: 0.0574 - accuracy: 0.9794 - precision: 0.9795 - recall: 0.9794 - f1: 0.9794\n",
      "Epoch 4/10\n",
      "4150/4150 [==============================] - 822s 198ms/step - loss: 0.0359 - val_loss: 0.0624 - accuracy: 0.9796 - precision: 0.9796 - recall: 0.9796 - f1: 0.9796\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 22:28:20.309079: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4149/4150 [============================>.] - ETA: 0s - loss: 0.0296"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/repository/python/virtualenvs/text_class/lib/python3.12/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4150/4150 [==============================] - 826s 199ms/step - loss: 0.0296 - val_loss: 0.0646 - accuracy: 0.9803 - precision: 0.9803 - recall: 0.9803 - f1: 0.9803\n",
      "Epoch 6/10\n",
      "4150/4150 [==============================] - 819s 197ms/step - loss: 0.0244 - val_loss: 0.0695 - accuracy: 0.9797 - precision: 0.9797 - recall: 0.9797 - f1: 0.9797\n",
      "Epoch 7/10\n",
      "4150/4150 [==============================] - 800s 193ms/step - loss: 0.0214 - val_loss: 0.0722 - accuracy: 0.9791 - precision: 0.9791 - recall: 0.9791 - f1: 0.9791\n",
      "Epoch 8/10\n",
      "4150/4150 [==============================] - 824s 199ms/step - loss: 0.0185 - val_loss: 0.0743 - accuracy: 0.9791 - precision: 0.9791 - recall: 0.9791 - f1: 0.9791\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 23:22:49.246426: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4150/4150 [==============================] - 814s 196ms/step - loss: 0.0152 - val_loss: 0.0813 - accuracy: 0.9785 - precision: 0.9785 - recall: 0.9785 - f1: 0.9785\n",
      "Epoch 10/10\n",
      "4149/4150 [============================>.] - ETA: 0s - loss: 0.0145"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/repository/python/virtualenvs/text_class/lib/python3.12/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4150/4150 [==============================] - 806s 194ms/step - loss: 0.0145 - val_loss: 0.0826 - accuracy: 0.9787 - precision: 0.9787 - recall: 0.9787 - f1: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7efc81f92690>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "model.fit(\n",
    "    tf_train_set,\n",
    "    validation_data=tf_validation_set,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b32de-b655-483b-851e-15642975595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### predict on a text\n",
    "## imports only for new session\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# 1. tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nreimers/MiniLM-L3-H384-uncased\")\n",
    "\n",
    "# 2. Model\n",
    "model_path = \"checkpoint/model_epoch_005.keras\"\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# 3. Function for prediction\n",
    "def predict_news(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
    "    \n",
    "    # Make prediction\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1).numpy()[0]\n",
    "    predicted_class = np.argmax(probabilities)\n",
    "    \n",
    "    # Get the class label\n",
    "    id2label = {0: \"TRUE\", 1: \"FAKE\"}\n",
    "    predicted_label = id2label[predicted_class]\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        \"label\": predicted_label,\n",
    "        \"confidence\": float(probabilities[predicted_class]),\n",
    "        \"probabilities\": {\n",
    "            \"TRUE\": float(probabilities[0]),\n",
    "            \"FAKE\": float(probabilities[1])\n",
    "        }\n",
    "    }\n",
    "\n",
    "# 4. How to use this function:\n",
    "# provide input: can be only one string, or a list with multiple strings\n",
    "example_texts = [\n",
    "    \"Scientists discover new treatment for cancer that shows promising results in clinical trials.\",\n",
    "    \"Breaking: Famous celebrity secretly an alien, government documents reveal shocking truth!\"\n",
    "]\n",
    "# iterate through list and make predictions:\n",
    "for text in example_texts:\n",
    "    result = predict_news(text)\n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    print(f\"Prediction: {result['label']} (confidence: {result['confidence']:.2f})\")\n",
    "    print(f\"TRUE probability: {result['probabilities']['TRUE']:.2f}\")\n",
    "    print(f\"FAKE probability: {result['probabilities']['FAKE']:.2f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# For other type of inputs:\n",
    "\n",
    "# # 5. Batch prediction function for multiple texts\n",
    "# def predict_batch(texts, batch_size=16):\n",
    "#     # Tokenize all texts\n",
    "#     inputs = tokenizer(texts, return_tensors=\"tf\", truncation=True, padding=True)\n",
    "    \n",
    "#     # Make predictions\n",
    "#     outputs = model(inputs)\n",
    "#     logits = outputs.logits\n",
    "#     probabilities = tf.nn.softmax(logits, axis=-1).numpy()\n",
    "#     predicted_classes = np.argmax(probabilities, axis=1)\n",
    "    \n",
    "#     # Format results\n",
    "#     id2label = {0: \"TRUE\", 1: \"FAKE\"}\n",
    "#     results = []\n",
    "    \n",
    "#     for i, pred_class in enumerate(predicted_classes):\n",
    "#         results.append({\n",
    "#             \"text\": texts[i][:100] + \"...\" if len(texts[i]) > 100 else texts[i],\n",
    "#             \"label\": id2label[pred_class],\n",
    "#             \"confidence\": float(probabilities[i][pred_class]),\n",
    "#             \"true_prob\": float(probabilities[i][0]),\n",
    "#             \"fake_prob\": float(probabilities[i][1])\n",
    "#         })\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # 6. Example for loading and predicting on a CSV file\n",
    "# def predict_from_csv(csv_file, text_column=\"text\"):\n",
    "#     # Load data\n",
    "#     df = pd.read_csv(csv_file)\n",
    "#     texts = df[text_column].tolist()\n",
    "    \n",
    "#     # Make predictions in batches\n",
    "#     all_results = []\n",
    "#     batch_size = 16\n",
    "    \n",
    "#     for i in range(0, len(texts), batch_size):\n",
    "#         batch_texts = texts[i:i+batch_size]\n",
    "#         batch_results = predict_batch(batch_texts)\n",
    "#         all_results.extend(batch_results)\n",
    "    \n",
    "#     # Convert to DataFrame and save\n",
    "#     results_df = pd.DataFrame(all_results)\n",
    "#     results_df.to_csv(\"prediction_results.csv\", index=False)\n",
    "#     print(f\"Saved predictions for {len(results_df)} texts to prediction_results.csv\")\n",
    "    \n",
    "#     return results_df\n",
    "\n",
    "# df = predict_from_csv(\"your_news_file.csv\", text_column=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57689e-697d-4dd8-a93f-031ec3fc315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model FineTuned after 5 epochs\n",
    "# Text: Scientists discover new treatment for cancer that ...\n",
    "# Prediction: FAKE (confidence: 0.91)\n",
    "# TRUE probability: 0.09\n",
    "# FAKE probability: 0.91\n",
    "# --------------------------------------------------\n",
    "# Text: Breaking: Famous celebrity secretly an alien, gove...\n",
    "# Prediction: FAKE (confidence: 1.00)\n",
    "# TRUE probability: 0.00\n",
    "# FAKE probability: 1.00\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Model FineTuned after 10 epochs\n",
    "# Text: Scientists discover new treatment for cancer that ...\n",
    "# Prediction: FAKE (confidence: 0.99)\n",
    "# TRUE probability: 0.01\n",
    "# FAKE probability: 0.99\n",
    "# --------------------------------------------------\n",
    "# Text: Breaking: Famous celebrity secretly an alien, gove...\n",
    "# Prediction: FAKE (confidence: 1.00)\n",
    "# TRUE probability: 0.00\n",
    "# FAKE probability: 1.00\n",
    "# --------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
